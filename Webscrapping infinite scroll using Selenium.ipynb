{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only use Selenium for extracting href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    " \n",
    "# def scrape_url(base_url):\n",
    " \n",
    "#     # Load WebDriver and navigate to the page url.\n",
    "#     # This will open a browser window.\n",
    "#     driver = webdriver.Chrome(\"C:/Users/Gabriela/Programming/courses/Python_mega_course_app4_to_10/app7_Real_Estate_Webscraper/chromedriver.exe\")\n",
    "#     driver.get(base_url)\n",
    "     \n",
    "#     #initiate list to store property-specififc URLs in    \n",
    "#     urls = []\n",
    " \n",
    "#     #  get number of results --> used to determine over how many pages to iterate\n",
    "#     property_nr = int(driver.find_element_by_class_name('results-label').get_attribute('data-count'))\n",
    "#     driver.quit()\n",
    "#     print(f\"Number of properties: {property_nr}\")\n",
    "    \n",
    "#     # calculate number of pages to scroll over and round up float \n",
    "#     total_pages = math.ceil(property_nr/100) #Note use math.ceil() instead of np.ceil() to return int instead of float\n",
    "    \n",
    "#     #iterate over total_pages and add info to base_URL\n",
    "#     for i in range(1,total_pages+1):\n",
    "#         url = base_url + f\"&p={i}\"\n",
    "#         driver = webdriver.Chrome(\"C:/Users/Gabriela/Programming/courses/Python_mega_course_app4_to_10/app7_Real_Estate_Webscraper/chromedriver.exe\")\n",
    "#         driver.get(url)\n",
    "        \n",
    "#         #initiate scroll_count in order to avoid infinite loop on final page\n",
    "#         scroll_count =0\n",
    "        \n",
    "#         # First scroll to the end of the table by sending Page Down keypresses to\n",
    "#         # the browser window.\n",
    "#         print(f'Scrolling through page {i} of {total_pages}')\n",
    "#         while driver.find_elements_by_tag_name('h3')[-1].text != \"You've Seen the next 100 properties.\" \\\n",
    "#         and driver.find_elements_by_tag_name('h3')[-1].text != \"You've Seen the first 100 properties.\" \\\n",
    "#         and scroll_count < 200:\n",
    "#             # Find the first element on the page, so we can scroll down using the\n",
    "#             # element object's send_keys() method\n",
    "#             scroll_count +=1\n",
    "#             elem = driver.find_element_by_class_name('listing-price')\n",
    "#             elem.send_keys(Keys.PAGE_DOWN)\n",
    "\n",
    "#         # Once the whole table has loaded, grab all the visible links. \n",
    "#         print(f'Saving property URLs for page {i} to list.')\n",
    "#         visible_links = driver.find_elements_by_class_name('listing-price')\n",
    "#         for link in visible_links:\n",
    "#             urls.append(link.get_attribute('href'))\n",
    "#         if i == total_pages:\n",
    "#             pass\n",
    "#         else:\n",
    "#             print(f\"{len(urls)} properties stored to list.\")\n",
    "\n",
    "#         driver.quit()\n",
    "    \n",
    "#     print(f\"Done. {len(urls)} property URLs saved to list.\")\n",
    "    \n",
    "#     return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Selenium to extract more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    " \n",
    "def scrape_url(base_url):\n",
    "     \n",
    "    #start timer\n",
    "    start = datetime.now()\n",
    "        \n",
    "    # Load WebDriver and navigate to the page url.\n",
    "    # This will open a browser window.\n",
    "    driver = webdriver.Chrome(\"C:/Users/Gabriela/Programming/courses/Python_mega_course_app4_to_10/app7_Real_Estate_Webscraper/chromedriver.exe\")\n",
    "    driver.get(base_url)\n",
    "     \n",
    "    #initiate list to store property-specififc URLs in    \n",
    "    urls = []\n",
    " \n",
    "    #  get number of results --> used to determine over how many pages to iterate\n",
    "    property_nr = int(driver.find_element_by_class_name('results-label').get_attribute('data-count'))\n",
    "    driver.quit()\n",
    "    print(f\"Number of properties: {property_nr}\")\n",
    "    \n",
    "    # calculate number of pages to scroll over and round up float \n",
    "    total_pages = math.ceil(property_nr/100) #Note use math.ceil() instead of np.ceil() to return int instead of float\n",
    "    \n",
    "    #iterate over total_pages and add info to base_URL\n",
    "    for i in range(1,total_pages+1):\n",
    "        url = base_url + f\"&p={i}\"\n",
    "        driver = webdriver.Chrome(\"C:/Users/Gabriela/Programming/courses/Python_mega_course_app4_to_10/app7_Real_Estate_Webscraper/chromedriver.exe\")\n",
    "        driver.get(url)\n",
    "        \n",
    "        #initiate scroll_count in order to avoid infinite loop on final page\n",
    "        scroll_count =0\n",
    "        \n",
    "        # First scroll to the end of the table by sending Page Down keypresses to\n",
    "        # the browser window.\n",
    "        print(f'Scrolling through page {i} of {total_pages}')\n",
    "        while driver.find_elements_by_tag_name('h3')[-1].text != \"You've Seen the next 100 properties.\" \\\n",
    "        and driver.find_elements_by_tag_name('h3')[-1].text != \"You've Seen the first 100 properties.\" \\\n",
    "        and scroll_count < 200:\n",
    "            # Find the first element on the page, so we can scroll down using the\n",
    "            # element object's send_keys() method\n",
    "            scroll_count +=1\n",
    "            elem = driver.find_element_by_class_name('listing-price')\n",
    "            elem.send_keys(Keys.PAGE_DOWN)\n",
    "\n",
    "        # Once the whole table has loaded, grab all the visible links. \n",
    "        print(f'Saving property URLs for page {i} to list.')\n",
    "        visible_links = driver.find_elements_by_class_name('property-card-primary-info')\n",
    "        \n",
    "        for link in visible_links:\n",
    "            info = {}\n",
    "            #get mandatory information\n",
    "            href = link.find_element_by_class_name(\"listing-price\").get_attribute('href')\n",
    "            \n",
    "            price = link.find_element_by_class_name(\"listing-price\").text.strip()\n",
    "            try:\n",
    "                price = int(''.join(c for c in price if c.isdigit())) #string non-numeric symbold and convert price to int\n",
    "            except:\n",
    "                price = price\n",
    "                \n",
    "            try:\n",
    "                street = link.find_element_by_class_name(\"property-address\").get_attribute('title')\n",
    "            except:\n",
    "                street = \"-\"\n",
    "            try:\n",
    "                location = link.find_element_by_class_name(\"property-city\").text.strip()\n",
    "                neighborhood = location.split()[0]\n",
    "                zip_ = location.split()[-1]\n",
    "            except:\n",
    "                location = \"-\"\n",
    "                neighborhood = \"-\"\n",
    "                zip_ = \"-\"\n",
    "              \n",
    "            #get optional information\n",
    "            try:\n",
    "                beds = int(link.find_element_by_class_name(\"property-beds\").find_element_by_tag_name(\"strong\").text)\n",
    "            except:\n",
    "                beds = \"-\"\n",
    "            try:\n",
    "                baths = int(link.find_element_by_class_name(\"property-baths\").find_element_by_tag_name(\"strong\").text)\n",
    "            except:\n",
    "                baths = \"-\"\n",
    "            try:\n",
    "                size = int(link.find_element_by_class_name(\"property-sqft\").find_element_by_tag_name(\"strong\").text)\n",
    "            except:\n",
    "                size = \"-\"\n",
    "\n",
    "                \n",
    "            #add information to info-dict\n",
    "            info['href'] = href\n",
    "            info['Price'] = price\n",
    "            info['Bedrooms'] = beds\n",
    "            info['Bathrooms'] = baths\n",
    "            info['Size'] = size\n",
    "            info['Street'] = street\n",
    "            info['zip'] = zip_\n",
    "            info['address'] = str(street) + ', ' + str(location)\n",
    "            \n",
    "            #append to urls-list\n",
    "            urls.append(info)\n",
    "        if i == total_pages:\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"{len(urls)} properties stored to list.\")\n",
    "\n",
    "        driver.quit()\n",
    "    \n",
    "    print(f\"Done. {len(urls)} property URLs saved to list.\")\n",
    "    \n",
    "    #end timer\n",
    "    end = datetime.now()\n",
    "    #calculate elapsed time\n",
    "    print(f\"\\nScraping took {end-start}.\")\n",
    "    \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of properties: 807\n",
      "Scrolling through page 1 of 9\n",
      "Saving property URLs for page 1 to list.\n",
      "100 properties stored to list.\n",
      "Scrolling through page 2 of 9\n",
      "Saving property URLs for page 2 to list.\n",
      "200 properties stored to list.\n",
      "Scrolling through page 3 of 9\n",
      "Saving property URLs for page 3 to list.\n",
      "300 properties stored to list.\n",
      "Scrolling through page 4 of 9\n",
      "Saving property URLs for page 4 to list.\n",
      "400 properties stored to list.\n",
      "Scrolling through page 5 of 9\n",
      "Saving property URLs for page 5 to list.\n",
      "500 properties stored to list.\n",
      "Scrolling through page 6 of 9\n",
      "Saving property URLs for page 6 to list.\n"
     ]
    }
   ],
   "source": [
    "urls = scrape_url(\"https://www.century21.com/real-estate/brooklyn-ny/LCNYBROOKLYN/?sa=CNYASTORIA%2CCNYBREEZYPOINT%2CCNYELMHURST%2CCNYHOWARDBEACH%2CCNYJACKSONHEIGHTS%2CCNYLONGISLANDCITY%2CCNYMASPETH%2CCNYMIDDLEVILLAGE%2CCNYOZONEPARK%2CCNYREGOPARK%2CCNYRIDGEWOOD%2CCNYSUNNYSIDE%2CCNYWOODHAVEN%2CCNYWOODSIDE&pr=%7C350000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'href': 'https://www.century21.com/property/7401-4th-avenue-e7-brooklyn-ny-11209-CBR51643402',\n",
       "  'Price': 320000,\n",
       "  'Bedrooms': 1,\n",
       "  'Bathrooms': 1,\n",
       "  'Size': nan,\n",
       "  'Street': '7401 4th Avenue #E7',\n",
       "  'zip': '11209',\n",
       "  'address': '7401 4th Avenue #E7, Brooklyn NY 11209'},\n",
       " {'href': 'https://www.century21.com/property/1170-ocean-parkway-8-g-brooklyn-ny-11230-REN014069992',\n",
       "  'Price': 205000,\n",
       "  'Bedrooms': 1,\n",
       "  'Bathrooms': 1,\n",
       "  'Size': nan,\n",
       "  'Street': '1170 Ocean Parkway 8-G',\n",
       "  'zip': '11230',\n",
       "  'address': '1170 Ocean Parkway 8-G, Brooklyn NY 11230'},\n",
       " {'href': 'https://www.century21.com/property/86-29-shore-parkway-unit-0008-howard-beach-ny-11414-C2182174151',\n",
       "  'Price': 315000,\n",
       "  'Bedrooms': 2,\n",
       "  'Bathrooms': 1,\n",
       "  'Size': nan,\n",
       "  'Street': '86-29 Shore Parkway Unit 0008',\n",
       "  'zip': '11414',\n",
       "  'address': '86-29 Shore Parkway Unit 0008, Howard Beach NY 11414'}]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(urls))\n",
    "urls[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use requests and BeautifulSoup to get more specific information from property-hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'href': 'https://www.century21.com/property/86-29-shore-parkway-unit-0008-howard-beach-ny-11414-C2182174151', 'Price': 315000, 'Bedrooms': 2, 'Bathrooms': 1, 'Size': '-', 'Street': '86-29 Shore Parkway Unit 0008', 'zip': '11414', 'address': '86-29 Shore Parkway Unit 0008, Howard Beach NY 11414', 'Property Features': 'Appliances: Dryer, Range / Oven, Refrigerator, Washer\\nFlooring: Hardwood\\nSewer: City\\nStyle: Other Style\\nWater: City Water\\n'}\n"
     ]
    }
   ],
   "source": [
    "def info_to_table(info_dict):\n",
    "    url = info_dict['href']\n",
    "    # get property-website content\n",
    "    r = requests.get(url, headers={'User-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:61.0) Gecko/20100101 Firefox/61.0'})\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    \n",
    "    #check if listing is for sale\n",
    "    if soup.find(\"div\", {\"class\":\"pdp-listing-type\"}).text.lower() == 'for sale' \\\n",
    "        or soup.find(\"div\", {\"class\":\"pdp-listing-type\"}).text.lower() == 'sale':\n",
    "\n",
    "        #get additional information\n",
    "        features = soup.find(\"ul\", {\"class\":\"property-features\"}).find_all(\"li\")\n",
    "        #iterate over features and extract info\n",
    "        feats = \"\"\n",
    "        for feature in features:\n",
    "            feats += feature.text + \"\\n\" \n",
    "        info_dict['Property Features'] = feats\n",
    "\n",
    "        return(info_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Save information in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for prop in info_dict[:6]:\n",
    "    new = info_to_table(info_dict)\n",
    "    try:\n",
    "        df.append(new, ingnore_index=True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebElement' object has no attribute 'ext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-211-d30721316522>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/Gabriela/Programming/courses/Python_mega_course_app4_to_10/app7_Real_Estate_Webscraper/chromedriver.exe\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://www.century21.com/property/7401-4th-avenue-e7-brooklyn-ny-11209-CBR51643402'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_class_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pdp-info-price'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_tag_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebElement' object has no attribute 'ext'"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(\"C:/Users/Gabriela/Programming/courses/Python_mega_course_app4_to_10/app7_Real_Estate_Webscraper/chromedriver.exe\")\n",
    "driver.get('https://www.century21.com/property/7401-4th-avenue-e7-brooklyn-ny-11209-CBR51643402')\n",
    "print(driver.find_element_by_class_name('pdp-info-price').find_element_by_tag_name('h5').ext)\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
